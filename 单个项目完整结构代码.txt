é¡¹ç›® 'kimi-ai-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ kimi-ai-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ base_provider.py
            ğŸ“„ kimi_ai_provider.py
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# [è‡ªåŠ¨å¡«å……] kimi-ai-2api ç”Ÿäº§ç¯å¢ƒé…ç½®
# è¯¥æ–‡ä»¶ç”± Project Chimera è‡ªåŠ¨ç”Ÿæˆï¼Œå¯ç›´æ¥ç”¨äºä¸€é”®éƒ¨ç½²ã€‚

# --- å®‰å…¨é…ç½® ---
# ç”¨äºä¿æŠ¤æ‚¨çš„ API æœåŠ¡çš„è®¿é—®å¯†é’¥ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ä¸ºæ‚¨è‡ªå·±çš„å¤æ‚å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088

# --- ä¼šè¯ç®¡ç† ---
# å¯¹è¯å†å²åœ¨å†…å­˜ä¸­çš„ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
SESSION_CACHE_TTL=3600


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# kimi-ai-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-kimi-ai-2api-default-key-please-change-me

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088

# --- ä¼šè¯ç®¡ç† (å¯é€‰) ---
# å¯¹è¯å†å²åœ¨å†…å­˜ä¸­çš„ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
SESSION_CACHE_TTL=3600


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for kimi-ai-2api (v1.0 - Genesis Edition)
# ====================================================================

FROM python:3.10-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ– (cloudscraper å¯èƒ½éœ€è¦)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¹¶åˆ‡æ¢åˆ°é root ç”¨æˆ·
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£å¹¶å¯åŠ¨ (ä½¿ç”¨å¤šä¸ª worker ä»¥å¤„ç†å¹¶å‘è¯·æ±‚)
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

services:
  nginx:
    image: nginx:latest
    container_name: kimi-ai-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8088}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - kimi-ai-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: kimi-ai-2api-app
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - kimi-ai-net

networks:
  kimi-ai-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import logging
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse

from app.core.config import settings
from app.providers.kimi_ai_provider import KimiAIProvider

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å…¨å±€ Provider å®ä¾‹ ---
provider: Optional[KimiAIProvider] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global provider
    logger.info(f"åº”ç”¨å¯åŠ¨ä¸­... {settings.APP_NAME} v{settings.APP_VERSION}")
    provider = KimiAIProvider()
    await provider.initialize()
    logger.info("æœåŠ¡å·²è¿›å…¥ 'Cloudscraper & Stateful Context' æ¨¡å¼ã€‚")
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    logger.info("åº”ç”¨å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

# --- å®‰å…¨ä¾èµ– ---
async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY != "1":
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

# --- API è·¯ç”± ---
@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request) -> StreamingResponse:
    try:
        request_data = await request.json()
        return await provider.chat_completion(request_data)
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/v1/models", dependencies=[Depends(verify_api_key)], response_class=JSONResponse)
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    return {"message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream kimi_ai_backend {
        # å…³é”®ï¼šä½¿ç”¨ ip_hash ç¡®ä¿æ¥è‡ªåŒä¸€å®¢æˆ·ç«¯çš„è¯·æ±‚è¢«è½¬å‘åˆ°åŒä¸€ä¸ª worker,
        # è¿™å¯¹äºä¿æŒå†…å­˜ä¸­ä¼šè¯ä¸Šä¸‹æ–‡çš„è¿ç»­æ€§è‡³å…³é‡è¦ã€‚
        ip_hash;
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://kimi_ai_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # æµå¼ä¼ è¾“ä¼˜åŒ–
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
pydantic-settings
python-dotenv
cloudscraper
cachetools
httpx
loguru
beautifulsoup4


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "kimi-ai-2api"
    APP_VERSION: str = "1.0.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† kimi-ai.chat è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼ API çš„é«˜æ€§èƒ½ä»£ç†ã€‚"

    API_MASTER_KEY: Optional[str] = None
    
    API_REQUEST_TIMEOUT: int = 180
    NGINX_PORT: int = 8088
    SESSION_CACHE_TTL: int = 3600

    KNOWN_MODELS: List[str] = ["kimi-k2-instruct-0905", "kimi-k2-instruct"]
    DEFAULT_MODEL: str = "kimi-k2-instruct-0905"
    
    UPSTREAM_URL: str = "https://kimi-ai.chat/wp-admin/admin-ajax.php"
    CHAT_PAGE_URL: str = "https://kimi-ai.chat/chat/"

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(self, request_data: Dict[str, Any]) -> StreamingResponse:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\kimi_ai_provider.py ---

import json
import time
import uuid
import random
import string
import re
import asyncio
from typing import Dict, Any, AsyncGenerator, Optional, List

import cloudscraper
from fastapi import HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from cachetools import TTLCache
from loguru import logger

from app.core.config import settings
from app.providers.base_provider import BaseProvider
from app.utils.sse_utils import create_sse_data, create_chat_completion_chunk, DONE_CHUNK

class KimiAIProvider(BaseProvider):
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        # ç¼“å­˜ç»“æ„: { "user_key": {"kimi_session_id": "...", "messages": [...] } }
        self.session_cache = TTLCache(maxsize=1024, ttl=settings.SESSION_CACHE_TTL)
        self._nonce: Optional[str] = None
        self._nonce_lock = asyncio.Lock()

    async def initialize(self):
        """åœ¨æœåŠ¡å¯åŠ¨æ—¶é¢„å–ä¸€æ¬¡ nonceã€‚"""
        logger.info("æ­£åœ¨åˆå§‹åŒ– KimiAIProviderï¼Œé¦–æ¬¡è·å– nonce...")
        await self._get_nonce()

    async def _fetch_nonce(self) -> str:
        """ä»èŠå¤©é¡µé¢æŠ“å–åŠ¨æ€çš„ nonce å€¼ã€‚"""
        try:
            logger.info("æ­£åœ¨ä»ä¸Šæ¸¸é¡µé¢æŠ“å–æ–°çš„ nonce...")
            response = self.scraper.get(settings.CHAT_PAGE_URL, timeout=20)
            response.raise_for_status()
            html_content = response.text

            match = re.search(r'var kimi_ajax = ({.*?});', html_content)
            if not match:
                raise ValueError("åœ¨é¡µé¢ HTML ä¸­æœªæ‰¾åˆ° 'kimi_ajax' JS å˜é‡ã€‚")

            ajax_data = json.loads(match.group(1))
            nonce = ajax_data.get("nonce")
            if not nonce:
                raise ValueError("'kimi_ajax' å¯¹è±¡ä¸­ç¼ºå°‘ 'nonce' å­—æ®µã€‚")
            
            logger.success(f"æˆåŠŸæŠ“å–åˆ°æ–°çš„ nonce: {nonce}")
            return nonce
        except Exception as e:
            logger.error(f"æŠ“å– nonce å¤±è´¥: {e}", exc_info=True)
            raise HTTPException(status_code=503, detail=f"æ— æ³•ä»ä¸Šæ¸¸æœåŠ¡è·å–å¿…è¦çš„åŠ¨æ€å‚æ•°: {e}")

    async def _get_nonce(self, force_refresh: bool = False) -> str:
        """è·å–å¹¶ç¼“å­˜ nonceï¼Œå¤„ç†å¹¶å‘è¯·æ±‚å’Œåˆ·æ–°é€»è¾‘ã€‚"""
        async with self._nonce_lock:
            if self._nonce is None or force_refresh:
                self._nonce = await self._fetch_nonce()
            return self._nonce

    def _get_or_create_session(self, user_key: str) -> Dict[str, Any]:
        """ä¸ºç”¨æˆ·è·å–æˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„ä¼šè¯å¯¹è±¡ã€‚"""
        if user_key in self.session_cache:
            return self.session_cache[user_key]
        
        timestamp = int(time.time() * 1000)
        random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=9))
        new_session_id = f"session_{timestamp}_{random_str}"
        
        new_session = {
            "kimi_session_id": new_session_id,
            "messages": []
        }
        self.session_cache[user_key] = new_session
        logger.info(f"ä¸ºç”¨æˆ· '{user_key}' åˆ›å»ºäº†æ–°çš„ä¼šè¯: {new_session_id}")
        return new_session

    def _build_contextual_prompt(self, history: List[Dict[str, str]], new_message: str) -> str:
        """
        å°†å†å²è®°å½•å’Œæ–°æ¶ˆæ¯æ‹¼æ¥æˆä¸€ä¸ªå•ä¸€çš„å­—ç¬¦ä¸²ï¼Œå¹¶æ ¹æ®éœ€è¦æˆªæ–­ä»¥æ»¡è¶³1000å­—ç¬¦çš„é™åˆ¶ã€‚
        """
        # æ ¼å¼åŒ–å†å²è®°å½•ä¸º "è§’è‰²: å†…å®¹" çš„å½¢å¼
        history_lines = []
        for msg in history:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "æ¨¡å‹"
            history_lines.append(f"{role}: {msg.get('content', '')}")
        
        history_str = "\n".join(history_lines)
        
        # åˆå§‹å®Œæ•´ä¸Šä¸‹æ–‡
        full_prompt = f"{history_str}\nç”¨æˆ·: {new_message}".strip()

        # æ™ºèƒ½æˆªæ–­é€»è¾‘ï¼šå¦‚æœè¶…å‡ºé•¿åº¦ï¼Œä»å¤´å¼€å§‹ç§»é™¤ä¸€è½®å¯¹è¯ï¼ˆç”¨æˆ·+æ¨¡å‹ï¼‰
        while len(full_prompt) > settings.CONTEXT_MAX_LENGTH and history:
            logger.warning(f"ä¸Šä¸‹æ–‡è¶…é•¿ ({len(full_prompt)} > {settings.CONTEXT_MAX_LENGTH})ï¼Œæ­£åœ¨ä»å¤´éƒ¨æˆªæ–­...")
            # ç§»é™¤æœ€æ—©çš„ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            history.pop(0) 
            # å¦‚æœè¿˜æœ‰æ¶ˆæ¯ï¼Œå†ç§»é™¤ä¸€æ¡å¯¹åº”çš„æ¨¡å‹æ¶ˆæ¯
            if history:
                history.pop(0)
            
            history_lines = []
            for msg in history:
                role = "ç”¨æˆ·" if msg.get("role") == "user" else "æ¨¡å‹"
                history_lines.append(f"{role}: {msg.get('content', '')}")
            history_str = "\n".join(history_lines)
            full_prompt = f"{history_str}\nç”¨æˆ·: {new_message}".strip()
        
        return full_prompt

    async def chat_completion(self, request_data: Dict[str, Any]) -> StreamingResponse:
        user_key = request_data.get("user")
        
        messages = request_data.get("messages", [])
        if not messages or messages[-1].get("role") != "user":
            raise HTTPException(status_code=400, detail="'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©ºï¼Œä¸”æœ€åä¸€æ¡å¿…é¡»æ˜¯ user è§’è‰²ã€‚")

        current_user_message = messages[-1]
        
        # æ ¹æ®æ˜¯å¦å­˜åœ¨ user_key å†³å®šå·¥ä½œæ¨¡å¼
        if user_key:
            # --- æœ‰çŠ¶æ€æ¨¡å¼ ---
            logger.info(f"æ£€æµ‹åˆ° 'user' å­—æ®µï¼Œè¿›å…¥æœ‰çŠ¶æ€æ¨¡å¼ã€‚ç”¨æˆ·: {user_key}")
            session_data = self._get_or_create_session(user_key)
            prompt_to_send = self._build_contextual_prompt(session_data["messages"], current_user_message["content"])
            kimi_session_id = session_data["kimi_session_id"]
        else:
            # --- æ— çŠ¶æ€æ¨¡å¼ ---
            logger.info("æœªæ£€æµ‹åˆ° 'user' å­—æ®µï¼Œè¿›å…¥æ— çŠ¶æ€æ¨¡å¼ã€‚")
            session_data = None # æ— çŠ¶æ€æ¨¡å¼ä¸‹æ²¡æœ‰ä¼šè¯æ•°æ®
            prompt_to_send = current_user_message["content"]
            # ä¸ºæ— çŠ¶æ€è¯·æ±‚ç”Ÿæˆä¸€æ¬¡æ€§çš„ session_id
            timestamp = int(time.time() * 1000)
            random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=9))
            kimi_session_id = f"session_{timestamp}_{random_str}"

        async def stream_generator() -> AsyncGenerator[bytes, None]:
            request_id = f"chatcmpl-{uuid.uuid4()}"
            model = request_data.get("model", settings.DEFAULT_MODEL)
            
            try:
                nonce = await self._get_nonce()
                payload = self._prepare_payload(prompt_to_send, model, kimi_session_id, nonce)
                
                logger.info(f"å‘ä¸Šæ¸¸å‘é€è¯·æ±‚, Session ID: {kimi_session_id}, æ¨¡å‹: {payload['model']}")
                logger.debug(f"å‘é€çš„å®Œæ•´ Prompt: {prompt_to_send}")
                
                response = self.scraper.post(settings.UPSTREAM_URL, data=payload, timeout=settings.API_REQUEST_TIMEOUT)
                response.raise_for_status()
                
                response_data = response.json()
                if not response_data.get("success"):
                    error_message = response_data.get("data", "æœªçŸ¥é”™è¯¯")
                    logger.warning(f"ä¸Šæ¸¸è¯·æ±‚å¤±è´¥: {error_message}ã€‚å¯èƒ½ nonce å¤±æ•ˆï¼Œæ­£åœ¨å°è¯•åˆ·æ–°å¹¶é‡è¯•...")
                    
                    nonce = await self._get_nonce(force_refresh=True)
                    payload['nonce'] = nonce
                    
                    response = self.scraper.post(settings.UPSTREAM_URL, data=payload, timeout=settings.API_REQUEST_TIMEOUT)
                    response.raise_for_status()
                    response_data = response.json()

                    if not response_data.get("success"):
                         raise HTTPException(status_code=502, detail=f"é‡è¯•åä¸Šæ¸¸è¯·æ±‚ä¾ç„¶å¤±è´¥: {response_data.get('data', 'æœªçŸ¥é”™è¯¯')}")

                assistant_response_content = response_data.get("data", {}).get("message", "")
                
                # å¦‚æœæ˜¯æœ‰çŠ¶æ€æ¨¡å¼ï¼Œåˆ™æ›´æ–°æœåŠ¡ç«¯ä¼šè¯å†å²
                if session_data:
                    session_data["messages"].append(current_user_message)
                    session_data["messages"].append({"role": "assistant", "content": assistant_response_content})
                    self.session_cache[user_key] = session_data
                    logger.info(f"ä¼šè¯ '{user_key}' ä¸Šä¸‹æ–‡å·²æ›´æ–°ã€‚")

                # åº”ç”¨ã€æ¨¡å¼ï¼šä¼ªæµå¼ç”Ÿæˆã€‘
                for char in assistant_response_content:
                    chunk = create_chat_completion_chunk(request_id, model, char)
                    yield create_sse_data(chunk)
                    await asyncio.sleep(0.02)

                final_chunk = create_chat_completion_chunk(request_id, model, "", "stop")
                yield create_sse_data(final_chunk)
                yield DONE_CHUNK

            except Exception as e:
                logger.error(f"å¤„ç†æµæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                error_message = f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}"
                error_chunk = create_chat_completion_chunk(request_id, model, error_message, "stop")
                yield create_sse_data(error_chunk)
                yield DONE_CHUNK

        return StreamingResponse(stream_generator(), media_type="text/event-stream")

    def _prepare_payload(self, prompt: str, model: str, session_id: str, nonce: str) -> Dict[str, Any]:
        # æ˜ å°„åˆ°ä¸Šæ¸¸æ¥å—çš„æ¨¡å‹åç§°
        if model == "kimi-k2-instruct-0905":
            upstream_model = "moonshotai/Kimi-K2-Instruct-0905"
        elif model == "kimi-k2-instruct":
            upstream_model = "moonshotai/Kimi-K2-Instruct"
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")

        return {
            "action": "kimi_send_message",
            "nonce": nonce,
            "message": prompt,
            "model": upstream_model,
            "session_id": session_id
        }

    async def get_models(self) -> JSONResponse:
        model_data = {
            "object": "list",
            "data": [
                {"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"}
                for name in settings.KNOWN_MODELS
            ]
        }
        return JSONResponse(content=model_data)


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

DONE_CHUNK = b"data: [DONE]\n\n"

def create_sse_data(data: Dict[str, Any]) -> bytes:
    """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚"""
    return f"data: {json.dumps(data)}\n\n".encode('utf-8')

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """
    åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æµå¼å—ã€‚
    """
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason
            }
        ]
    }



